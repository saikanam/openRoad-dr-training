# Offline RL Dataset Test Plan (`training_data/`)

This document outlines the tests to be performed on the raw CSV data located in the `../training_data/` directory before using it to construct the `MDPDataset` for training the CQL agent. The goal is to ensure data quality, consistency, and identify potential issues.

**Source:** Data generated by perturbation-based randomization around expert weights. Each file (`trainingdata_designName.csv`) corresponds to a specific design run.

## Test Categories

### 1. File Level Checks

*   **File Existence & Naming:** Verify all expected `trainingdata_*.csv` files are present and follow the naming convention.
*   **File Readability:** Confirm each CSV file can be successfully loaded (e.g., using `pandas.read_csv`). Check for encoding issues.
*   **Header Consistency:** Ensure all CSV files have the exact same header row (column names and order) as specified:
    ```
    uniqueID,designID,iteration,pin_count,net_count,drv,wireLength,drc_weight,marker_weight,fixed_weight,decay_weight,boxID,box_size,box_drv,L_N_box,L_N_drv,R_N_box,R_N_drv
    ```

### 2. Data Integrity & Consistency (Within Each File)

*   **Data Types:** Check if data types for each column are consistent with expectations (e.g., numeric types for counts, weights, DRVs; string/object types for IDs).
    *   Specifically check `box_drv`, `L_N_drv`, `R_N_drv` for non-numeric values (expecting numbers or potentially `None`/`NaN` for neighbors).
*   **Missing Values (General):** Identify columns with unexpected missing values (beyond `L_N_box`, `L_N_drv`, `R_N_box`, `R_N_drv` which can legitimately be missing for edge boxes). Document specific iterations/runs affected (e.g., the 6 known iterations).
*   **Iteration Consistency:** Within a single `uniqueID` and `iteration` group:
    *   Verify that `designID`, `pin_count`, `net_count`, `drv`, `wireLength`, `drc_weight`, `marker_weight`, `fixed_weight`, `decay_weight` are *constant* across all box rows.
*   **Box ID Uniqueness:** Within a single `uniqueID` and `iteration`, verify that `boxID` is unique for each row. Document the specific case (`bp_be_top_run_68`, iter 1) where duplicates exist and confirm if data differs.
*   **DRV Consistency:** Check if the iteration-level `drv` consistently differs from the sum of `box_drv` values within that same iteration. Analyze the pattern of this difference across iterations and designs.

### 3. Data Range & Validity Checks

*   **Iteration Numbers:** Check if `iteration` numbers are positive integers and generally sequential within each `uniqueID`.
*   **Counts:** Ensure `pin_count`, `net_count`, `box_drv` are non-negative.
*   **Weights:** Examine the range of values for `drc_weight`, `marker_weight`, `fixed_weight`, `decay_weight`. Are they within expected bounds? Check for extreme outliers resulting from the perturbation.
*   **DRV Values:** Check `drv`, `box_drv`, `L_N_drv`, `R_N_drv`. Ensure they are non-negative. Examine distributions for outliers.
*   **Wire Length:** Ensure `wireLength` is non-negative.
*   **Box Size:** Parse `box_size` (e.g., '10.50x10.50') and check if dimensions are positive.

### 4. Cross-File / Run Checks

*   **`uniqueID` vs `designID`:** Verify that each `uniqueID` corresponds to only one `designID` across all files.
*   **Design Metrics Consistency:** For a given `designID`, check if `pin_count` and `net_count` are consistent across different runs (`uniqueID`s) of that same design.

### 5. RL Suitability Checks (Predicting Training Potential)

These checks analyze the data distribution and dynamics to estimate how suitable the dataset is for offline RL and what kind of performance we might expect from the trained agent.

*   **Action Space Coverage & Distribution:**
    *   Visualize the distribution of each weight (`drc_weight`, `marker_weight`, `fixed_weight`, `decay_weight`) across the entire dataset (e.g., using histograms or density plots).
    *   Are the ranges wide enough due to the perturbations? Is the data heavily concentrated around the original expert weights, or is there significant exploration? Pay special attention to the `decay_weight` bias towards 1.0.
    *   Analyze correlations between the weights applied. Verify they were perturbed largely independently.
*   **State Space Coverage:**
    *   Visualize the distribution of key state features (e.g., `drv`, `total_box_drv`, `wireLength`, iteration number) across the dataset.
    *   Does the data cover a representative range of scenarios (high/low DRV, different stages of routing)? Note the skew towards low-DRV states.
*   **Reward Signal Analysis (Multi-Component):**
    *   Analyze the distribution of each planned reward component:
        *   `primary_reward = drv_{t-1} - drv_t` (Reduction in iteration-level DRV)
        *   `box_reward = total_box_drv_{t-1} - total_box_drv_t` (Reduction in the **sum** of DRVs across all boxes)
        *   `stuck_penalty` (calculated based on `drv_t`, `drv_{t-1}`, `drv_{t-2}`)
        *   `convergence_bonus`
    *   *Experimental/Potential Denser Signals:*
        *   `max_box_drv_reward = max_box_drv_{t-1} - max_box_drv_t` (Reduction in the maximum DRV on any single box)
        *   `num_violating_reward = num_boxes_drv>0_{t-1} - num_boxes_drv>0_t` (Reduction in the **count** of boxes with `box_drv > 0`)
    *   Are there sufficient examples of positive rewards (DRV reduction) in the primary/box components? Assess sparsity.
    *   Are the potential denser signals (`max_box_drv_reward`, `num_violating_reward`) actually denser (less peaked at zero) than the primary/box rewards?
    *   How often is the `stuck_penalty` applied? Is its magnitude appropriate?
    *   How often is the `convergence_bonus` applied (linked to termination)?
*   **Action-Reward Relationship (Preliminary):**
    *   Perform exploratory analysis (e.g., scatter plots, correlation checks) between applied weights (actions) and resulting rewards or DRV changes, potentially conditioning on parts of the state (e.g., current DRV level).
    *   Are there any visible trends suggesting that certain weight ranges lead to better outcomes in certain situations? (High noise is expected, but complete lack of correlation might be a concern, especially given reward sparsity).
*   **Trajectory Lengths & Termination:**
    *   Analyze the distribution of trajectory lengths (number of iterations per `uniqueID`).
    *   How many trajectories terminate with `drv == 0` versus hitting a maximum iteration limit?
*   **Data Density:**
    *   Assess qualitatively if there are enough valid transitions (~6000 observed) across different designs and iteration phases, especially considering reward sparsity. Very sparse data might limit learning.

## Tools

*   Python with libraries like `pandas`, `numpy`, `matplotlib`, `seaborn` for analysis and visualization.

## Reporting

Document any failures, inconsistencies, or suspicious patterns found during these tests. This will inform the cleaning and preprocessing steps during the construction of the `MDPDataset`. 