{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing Weight Optimization: Data Processing and Training Workflow\n",
    "\n",
    "This notebook runs the main workflow for the offline RL agent:\n",
    "1. **Setup:** Clones the repo (or pulls updates), installs dependencies, and sets up paths.\n",
    "2. **Data Preparation:** Runs the `create_dataset.py` script to generate the `MDPDataset`.\n",
    "3. **Model Training:** Runs the `train_cql.py` script to train the CQL agent.\n",
    "4. **Basic Results:** Loads and plots some training metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Google Colab Setup ===\n",
    "# Clone the repository (if not already done) and pull latest changes\n",
    "\n",
    "import os\n",
    "\n",
    "REPO_NAME = \"openRoad-dr-training\" # Your repository name\n",
    "# Use HTTPS URL for easier public/token access in Colab by default\n",
    "# You might need to generate a Personal Access Token (PAT) on GitHub \n",
    "# and use it in the URL like: https://<YOUR_PAT>@github.com/saikanam/openRoad-dr-training.git\n",
    "# Or configure SSH keys if you prefer.\n",
    "REPO_URL_HTTPS = \"https://github.com/saikanam/openRoad-dr-training.git\"\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Check if repo already exists in Colab's /content directory\n",
    "    colab_repo_path = f\"/content/{REPO_NAME}\"\n",
    "    if not os.path.exists(colab_repo_path):\n",
    "        print(f\"Cloning repository: {REPO_URL_HTTPS} into {colab_repo_path}\")\n",
    "        # Clone using HTTPS\n",
    "        !git clone {REPO_URL_HTTPS} {colab_repo_path}\n",
    "        %cd {colab_repo_path}\n",
    "    else:\n",
    "        print(f\"Repository {REPO_NAME} already exists in /content. Pulling latest changes...\")\n",
    "        %cd {colab_repo_path}\n",
    "        !git pull origin main\n",
    "else:\n",
    "    # Assume running locally, repository is the current directory\n",
    "    print(\"Running locally, repository assumed to be current directory.\")\n",
    "    # Optionally, you could still run git pull here if desired\n",
    "    # !git pull origin main \n",
    "    pass \n",
    "\n",
    "print(f\"\\nCurrent directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Install Dependencies ===\n",
    "print(\"Installing dependencies from requirements.txt...\")\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports and Path Definitions ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import d3rlpy # Verify import after install\n",
    "\n",
    "# Define relative paths (should work relative to repo root)\n",
    "DATA_INPUT_DIR = \"training_data\" # Relative path from create_dataset.py default\n",
    "DATA_OUTPUT_DIR = \"data\" \n",
    "DATASET_FILENAME = \"routing_dataset.h5\"\n",
    "DATASET_PATH = os.path.join(DATA_OUTPUT_DIR, DATASET_FILENAME)\n",
    "SCALER_PATH = os.path.join(DATA_OUTPUT_DIR, \"state_scaler.pkl\")\n",
    "LOG_DIR = \"d3rlpy_logs\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(DATA_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset will be saved to: {DATASET_PATH}\")\n",
    "print(f\"Scaler will be saved to: {SCALER_PATH}\")\n",
    "print(f\"Training logs will be saved in: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Run the script to process the raw CSV data into the `MDPDataset` format required by d3rlpy. This assumes the `training_data` directory exists at the root of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Run Data Creation Script ===\n",
    "# Note: This assumes the 'training_data' folder is present in the root \n",
    "# of the repository when cloned. It's not tracked by Git currently.\n",
    "\n",
    "if not os.path.exists(DATA_INPUT_DIR):\n",
    "    print(f\"Warning: Input data directory '{DATA_INPUT_DIR}' not found. \\n\",\n",
    "          f\"Please ensure it exists in the repository root ('{os.getcwd()}') before running this step.\")\n",
    "else:\n",
    "    print(f\"Running data creation script... Input: {DATA_INPUT_DIR}, Output Dir: {DATA_OUTPUT_DIR}\")\n",
    "    !python src/data_processing/create_dataset.py --input_dir {DATA_INPUT_DIR} --output_dir {DATA_OUTPUT_DIR} --output_filename {DATASET_FILENAME}\n",
    "\n",
    "    # === Verify Output ===\n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        print(f\"\\nDataset file created successfully at: {DATASET_PATH}\")\n",
    "    else:\n",
    "        print(f\"\\nError: Dataset file not found at {DATASET_PATH}. Check script output above.\")\n",
    "\n",
    "    if os.path.exists(SCALER_PATH):\n",
    "        print(f\"State scaler file created successfully at: {SCALER_PATH}\")\n",
    "    else:\n",
    "        print(f\"Error: State scaler file not found at {SCALER_PATH}. Check script output above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Run the training script using the generated dataset. Adjust hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training Configuration ===\n",
    "# Using parameters that showed some promise previously (low LR, low alpha, no reward scaling)\n",
    "EXPERIMENT_NAME = \"CQL_Colab_Run_v3\" # Increment experiment name\n",
    "CONSERVATIVE_WEIGHT = 1.0\n",
    "ACTOR_LR = 1e-6\n",
    "CRITIC_LR = 1e-6\n",
    "EPOCHS = 50 # Adjust as needed\n",
    "USE_REWARD_SCALER_FLAG = \"--no-use_reward_scaler\" # Use '--use_reward_scaler' to enable\n",
    "DEVICE_FLAG = \"--use_gpu\" # Use '--use_cpu' if no GPU available in Colab\n",
    "SEED = 42 # Set a random seed for reproducibility\n",
    "\n",
    "# === Run Training Script ===\n",
    "print(f\"Starting training run: {EXPERIMENT_NAME}\")\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"Error: Dataset file not found at {DATASET_PATH}. Cannot start training.\")\n",
    "else:\n",
    "    !python src/training/train_cql.py \\\n",
    "        --dataset {DATASET_PATH} \\\n",
    "        --experiment_name {EXPERIMENT_NAME} \\\n",
    "        --conservative_weight {CONSERVATIVE_WEIGHT} \\\n",
    "        --actor_lr {ACTOR_LR} \\\n",
    "        --critic_lr {CRITIC_LR} \\\n",
    "        --epochs {EPOCHS} \\\n",
    "        --seed {SEED} \\\n",
    "        {USE_REWARD_SCALER_FLAG} \\\n",
    "        {DEVICE_FLAG}\n",
    "        # Add other arguments as needed\n",
    "\n",
    "    print(f\"\\nTraining finished. Logs should be available in: {os.path.join(LOG_DIR, EXPERIMENT_NAME)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Results\n",
    "\n",
    "Load and plot some basic training metrics from the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Logs ===\n",
    "# Use the experiment name defined in the training cell\n",
    "log_path = os.path.join(LOG_DIR, EXPERIMENT_NAME) \n",
    "metrics_to_plot = [\n",
    "    \"critic_loss\", \"actor_loss\", \"temp_loss\", \"alpha_loss\", \n",
    "    \"conservative_loss\", \"td_error\", \"initial_state_value\", \n",
    "    \"temperature\", \"alpha\" # Include temp and alpha if available\n",
    "]\n",
    "\n",
    "metrics_data = {}\n",
    "print(f\"Attempting to load logs from: {log_path}\")\n",
    "\n",
    "if os.path.exists(log_path):\n",
    "    for metric in metrics_to_plot:\n",
    "        csv_path = os.path.join(log_path, f\"{metric}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            try:\n",
    "                metrics_data[metric] = pd.read_csv(csv_path)\n",
    "                print(f\"  Loaded {metric}.csv\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {metric}.csv: {e}\")\n",
    "        else:\n",
    "            # Don't print missing for optional ones like temp/alpha\n",
    "            if metric not in ['temperature', 'alpha']: \n",
    "                 print(f\"  {metric}.csv not found.\")\n",
    "else:\n",
    "    print(f\"Log directory not found: {log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot Metrics ===\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "num_metrics = len(metrics_data)\n",
    "\n",
    "if num_metrics > 0:\n",
    "    # Determine grid size (e.g., 3 columns)\n",
    "    ncols = 3\n",
    "    nrows = (num_metrics + ncols - 1) // ncols \n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 5 * nrows), squeeze=False)\n",
    "    axes = axes.flatten() # Flatten to easily iterate\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for metric, df in metrics_data.items():\n",
    "        if not df.empty:\n",
    "            ax = axes[plot_idx]\n",
    "            if 'step' in df.columns and 'value' in df.columns:\n",
    "                sns.lineplot(data=df, x='step', y='value', ax=ax)\n",
    "                ax.set_title(f\"{metric} vs. Training Step\")\n",
    "                ax.set_xlabel(\"Training Step\")\n",
    "                ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "            else:\n",
    "                ax.set_title(f\"{metric} - Data Missing Columns\")\n",
    "            plot_idx += 1\n",
    "        \n",
    "    # Hide unused subplots\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "        \n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No metric data loaded to plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
